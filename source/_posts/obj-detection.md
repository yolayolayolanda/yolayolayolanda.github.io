---
layout: poster
title: Object Detection Learning
date: 2019-04-14 14:12:54
categories: CV
mathjax: true
---

## Region based Object Detectors: R-CNN(Region CNN), Fast R-CNN, Faster R-CNN and R-FCN

###### [动手学深度学习第七课：物体检测](https://youtu.be/bqqlNu6byso)

[What do we learn from region based object detectors (Faster R-CNN, R-FCN, FPN)?](https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9)

<!-- more -->

- By [Jonathan Hui](https://medium.com/@jonathan_hui)
- Recommend 

#### Sliding-window detectors (brute force) Method

![Sliding Window](https://raw.githubusercontent.com/yolayolayolanda/yolayolayolanda.github.io/master/images/1*BYSA3iip3Cdr0L_x5r468A.png)

- Scan from left and right and from up to down with windows of varied sizes and aspect ratios  (the ratio of the width to the height).

- Wrap(Resize) the cut-out patches to fixed size.

- CNN classifier to extract features.

- Then use SVM to classify and use regressor to locate boundary box.

- Pseudo code:

- ```
  for window in windows
      patch = get_patch(image, window)
      results = detector(patch)
  ```

#### Selective Search Method

- **Region proposal** method to create **region of interest(ROIs)**.
- One pixel a group —> Combining closet groups with calculated similar texture to relatively small groups (try to avoid single region gobbling others) —> Continue merging groups until combined to regions

##### R-CNN

Region proposal method to create about 2000 ROIs.

[R-CNN for Object Detection - Washington University ](https://courses.cs.washington.edu/courses/cse590v/14au/cse590v_wk1_rcnn.pdf)

![R-CNN.1](https://raw.githubusercontent.com/yolayolayolanda/yolayolayolanda.github.io/master/images/image-20190414151111502.png)

**Fine tune**: remove the last softmax layer, remaining the other layers to output the feature directly. 

**Steps**:

![R-CNN.2](https://raw.githubusercontent.com/yolayolayolanda/yolayolayolanda.github.io/master/images/image-20190414151138150.png)

![R-CNN.3](https://raw.githubusercontent.com/yolayolayolanda/yolayolayolanda.github.io/master/images/1*Wmw21tBUez37bj-1ws7XEw.png)

1. Pre-train CNN for image classification.
2. Fine-tune CNN for object detection. 
3. Selective search every image (heuristic) to generate small patches.
4. Input every patch(wrapped) into the convolutional neural network(backbone), then output the features of the patch (eg:256)
5.  After FC layers, feed the features to
   1. Discriminator: to classify the object. Here they use different SVM to classify each object - if the obj is the one that we want 
   2. Regressor: to place a box at the location of the object. A box <==> A 4-dim vector ($x_{left-top}, y_{left-top}, x_{right-bottom}, y_{right-bottom}$).
6. Loss function:
   1. Classification loss
   2. Loss of box vector.(Distance between two vectors)

- Pseudo code:

  ```
  ROIs = region_proposal(image)
  for ROI in ROIs
      patch = get_patch(image, ROI)
      results = detector(patch)
  ```

#### Boundary Box Regression Method

Region proposal methods are computation intense. Add a linear regressor (FC layers) after region proposal.

##### Fast R-CNN

R-CNN needs many proposals (patches) to be accurate and many regions overlap with each other.

R-CNN forwards every picture 2000 times(2000 patches (proposals, ROIs) , one patch one time), which is too expensive.

![Fast R-CNN.1](https://raw.githubusercontent.com/yolayolayolanda/yolayolayolanda.github.io/master/images/1*Dd3-sugNKInTIv12u8cWkw.png)

**Steps: **(All steps are trained end-to-end with **multi-task losses** (classification loss and localization loss))

1. Use a **feature extractor** (s CNN) to firstly extract the feature map instead of scratches.

2. Combine the feature map with ROIs generated by external  region proposal method (eg: Selective Search). Thus they generate feature maps of the ROIs.

3. Wrap the patches: using **ROI pooling (Maximum pooling)**.

   ![Fast R-CNN.2](https://raw.githubusercontent.com/yolayolayolanda/yolayolayolanda.github.io/master/images/1*LLP4tKGsYGgAx3uPfmGdsw.png)

4. FC layers.

5. Softmax classification and regressor boundary box localization.

**Advantages**

​	Fast process time because of not repeating the feature extraction. (No overlap)

**Pseudo code**

```
feature_maps = cnn_process(image)
ROIs = region_proposal(image)
for ROI in ROIs
    patch = roi_pooling(feature_maps, ROI)
    results = detector2(patch)
```

##### Faster R-CNN

Although faster than R-CNN, Fast R-CNN still suffers a lot from the **external** region proposal (Fast R-CNN takes 2.3 seconds to make a prediction in which 2 seconds are for generating 2000 ROIs).

So, Faster R-CNN substitute the region proposal with an **internal** deep network, which is the region proposal network (**RPN**) (it only take 10ms per image).

![Faster R-CNN](https://cdn-images-1.medium.com/max/2400/1*0cxB2pAxQ0A7AhTl-YT2JQ.jpeg)

###### Region proposal network

![RPN](https://cdn-images-1.medium.com/max/2400/1*z0OHn89t0bOIHwoIOwNDtg.jpeg)

- ZF network is enough. VGG and ResNet can be used for more comprehensive images features at the cost of time.

- After 256 features maps extracted, use two branches 

  - One is regressor for predicting a boundary box (4 coordinates)
  - One is classificatory for measuring whether the box contains an object (2 scores).

- Note that, for each location in a feature map, RPN makes **k** guesses for what is the ROI contains this location.

  ![Faster R-CNN.RPN](https://cdn-images-1.medium.com/max/1600/1*smu6PiCx4LaPwGIo3HG0GQ.jpeg)

  So that's why after FC there are 4k coordinates for reg. and 2k scores for cls..

  - **k**: more candidates, more probability for correct object region.

  - If the k initial guesses have various shapes and sizes, it will be better to find the correct **one**. So they proposed **anchors and offsets** to constrain the prediction to resemble to the anchors but different:

    ![Faster R-CNN.RPN](https://cdn-images-1.medium.com/max/1600/1*yF_FrZAkXA3XKFA-sf7XZw.png)

    - **Anchors**: the reference box centered at each location, all locations the same shape anchors. So they are carefully pre-selected so they are diverse and cover real-life objects at different scales and aspect ratios reasonable well. ***Anchors*** *are also called* ***priors*** *or* ***default boundary boxes*** *in different papers.*

      ![Faster R-CNN.RPN](https://cdn-images-1.medium.com/max/1600/1*RJoauxGwUTF17ZANQmL8jw.png)

    - **Offsets**: the variances between the left-top coordinates of the prediction and the left-top coordinates of the anchor.

    - Faster R-CNN deploys 9 anchors boxes: 3 different scales at 3 different aspect ratio; 9 anchors, 9 guesses(predictions), which means k=9. 

    - This strategy makes early training more stable and easier.

**Performance**

![Comparison](https://cdn-images-1.medium.com/max/1600/1*fO2MSeQxIVVUUp6csJ8oWg.jpeg)

###### Pseudo code

```
feature_maps = process(image)
ROIs = region_proposal(feature_maps)
for ROI in ROIs
    patch = roi_pooling(feature_maps, ROI)
    class_scores, box = detector(patch)         # Expensive!
    class_probabilities = softmax(class_scores)
```

##### Region-based Fully Convolutional Networks (R-FCN)

###### [original paper](https://arxiv.org/pdf/1605.06409.pdf)

Idea: locating a face when an eye is detected. Thus, we don't need that much predictions(ROIs) in the Faster R-CNN, which is expensive. (See the pseudo code above) ==> **To reduce the cost of detecting target objects from ROIs**(whether in the target classes).

![R-FCN](https://cdn-images-1.medium.com/max/2400/1*Gv45peeSM2wRQEdaLG_YoQ.png)

In R-FCN, they add a convolutional network on the second branch (independent of ROIs) to do class detection: **score the regions of the feature maps on how much probability of them to belong to one target class**.

> In R-FCN, all learnable weight layers are convolutional and are computed on the entire image. The last convolutional layer produces a bank of $k^2​$ *position-sensitive score maps* for each category, and thus has a $k^2(C + 1)​$-channel output layer with C object categories ($+1​$
> for background). 

**Steps**:

1. By the added convolutional network (before the last layer), R-FCN generate $C+1​$ categories/channels/feature maps .

2. In the last convolutional layer, divide the categories/channels/feature maps into $k^2$ smaller regions. Eg: 3 $\times$ 3 regions.

   Thus, we get $k^2(C + 1)$ small-region feature maps, which are named **position-sensitive score maps**.

   ![R-FCN](https://cdn-images-1.medium.com/max/1600/1*HaOHsDYAf8LU2YQ7D3ymOg.png)

3. For ROIs, we divide them into 3 × 3 regions and ask how likely each region contains the corresponding part of the object.

   ![img](https://cdn-images-1.medium.com/max/1600/1*Ym6b1qS0pXpeRVMysvvukg.jpeg)

4. Then, we evaluate each ROI by mapping it to score maps, which is called **position-sensitive ROI-pool**

   ![R-FCN](https://cdn-images-1.medium.com/max/1600/1*K4brSqensF8wL5i6JV1Eig.png)

   And the score of this ROI belongs to one class is the average of the vote_array(**avg-pooling**):

   ![R-FCN](https://cdn-images-1.medium.com/max/1600/1*ZJiWcIl2DUyx1-ZqArw33A.png)But the authors said max-pooling is also fine.

###### Pseudo code

```
feature_maps = process(image)
ROIs = region_proposal(feature_maps)         
score_maps = compute_score_map(feature_maps)
for ROI in ROIs
    V = region_roi_pool(score_maps, ROI)     
    class_scores, box = average(V)                   # Much simpler!
    class_probabilities = softmax(class_scores)
```

------

## Single Shot Object Detectors: SSD and YOLO

[What do we learn from single shot object detectors (SSD, YOLOv3), FPN & Focal loss (RetinaNet)?    -- Jonathan Hui](https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d)

##### Single Shot detectors

**Idea**: like R-FCN, people want to reduce the work load of Faster R-CNN detecting the category of each ROI. But some are a bit "bolder". They want to derive both boundary boxes and classes directly from feature maps in one step:

```
feature_maps = process(image)
results = detector3(feature_maps) # No more separate step for ROIs
```

Considering the **sliding-window**, the fatal problem of it is using too much bounding boxes of different sizes and aspect-ratios to detect various objects.

![Single shot](https://cdn-images-1.medium.com/max/2400/1*tE6DUwv6VIHu1KlwYmSBTw.jpeg)

Single shot detector uses $k$ anchors as the initial sliding-window. Then detect the target and derive both the predicted boundary and the class simultaneously. 

###### Steps:

1. Scan the feature map by every location with anchors. (Just the same as Faster R-CNN)

   ![Single shot](https://cdn-images-1.medium.com/max/1600/1*1F8rWQyBV-P8pDn0Avx-OA.png)

   ***However, Faster R-CNN uses anchors and offsets to predict the ROIs (whether there is an object in the box). Here Single Shot uses it to directly predict the classification (the category to which the box belongs).***

2. Then we use the convolution filter to extract $4+C+1​$ parameters to describe the shape and the category of each prediction of a specific location {4 coordinates, C classes + 1 background }. 

###### Advantage

Real-time processing speed.

###### Drawback

Relatively low accuracy. Especially when detecting objects that are too close and small.

##### SSD

[original paper - SSD: Single Shot MultiBox Detector](https://arxiv.org/pdf/1512.02325.pdf)

Different from single shot prediction:
![Single shot](https://cdn-images-1.medium.com/max/2400/1*1C5hgYTdBvCdCYWbXEaVww.png)

SSD (Single Shot MultiBox Detector) uses **multi-scale feature maps** for prediction, which is good to modify convolution layers reducing spacial dimension and resolution.

![SSD](https://cdn-images-1.medium.com/max/2400/1*k0eFZw1jlF9xPvhzBKt6LQ.png)

From left to right:

1. VGG-16 + Blue convolutional layers - to extract the feature maps, as well as generate multi-scale feature maps (It's VGG-16 in the paper)
2. Green convolutional filters - to make prediction

![SSD](https://raw.githubusercontent.com/yolayolayolanda/yolayolayolanda.github.io/master/images/image-20190415221313455.png)

As we can see, the MultiBox works better if the resolution of the input image is relatively high, or it will be too hard for the deep layers to locate the small objects.

##### YOLO

[YOLO official website](https://pjreddie.com/darknet/yolo/)

[YOLO first paper](https://arxiv.org/pdf/1506.02640.pdf)

YOLO means: you only look once.

![YOLO](https://cdn-images-1.medium.com/max/2400/1*NBnDpz8fitkhcdnkgF2bvg.png)

**YOLO** uses DarkNet to make feature detection followed by convolutional layers.

It is different from SDD by the convolutional layers. It partially flattens features maps and concatenates it with another lower resolution maps. For example, YOLO reshapes a 28 × 28 × 512 layer to 14 × 14 × 2048. Then it concatenates with the 14 × 14 ×1024 feature maps. Afterward, YOLO applies convolution filters on the new 14 × 14 × 3072 layer to make predictions.

**YOLO9000** can detect 9000 categories of objects.

**YOLO (v2)** makes many implementation improvements to push the accuracy.

![Performance Comparison](https://cdn-images-1.medium.com/max/1600/1*NJj17Z6FgffYaA4WH2WIjw.png)

**YOLO (v3)**

YOLOv3 change to a more complex backbone for feature extraction. Darknet-53 mainly compose of 3 × 3 and 1× 1 filters with skip connections like the **residual blocks** in ResNet. Darknet-53 has less BFLOP (billion floating point operations) than ResNet-152, but achieves the same classification accuracy at 2x faster.

![YOLOv3](https://cdn-images-1.medium.com/max/1600/1*biRYJyCSv-UTbTQTa4Afqg.png)

YOLOv3 also adds **Feature Pyramid** (discussed next) to detect small objects better. Here is the accuracy and speed tradeoff for different detectors.

![performance comparison](https://cdn-images-1.medium.com/max/1600/1*rfj_5yjKZm2LJvVzMXmLFA.png)

------

## Feature Pyramid Detectors: FPN & Focal Loss(RetinaNet)

##### Feature Pyramid Networks (FPN)

Detecting objects in different scales is challenging in particular for small objects in single shot detectors. 

To improve accuracy and speed, FPN:
	Feature extractor —> **Higher quality feature map pyramid**

###### Data Flow

FPN composes of a **bottom-up** and a **top-down** pathway.

![FPN](https://cdn-images-1.medium.com/max/1600/1*aMRoAN7CtD1gdzTaZIT5gA.png)

- Bottom-up pathway

  - The usual convolutional network for feature extraction: deeper == lower resolution == higher level comprehension (semantic value)

    ![FPN - bottom-up](https://cdn-images-1.medium.com/max/1600/1*_kxgFskpRJ6bsxEjh9CH6g.jpeg)

    *SSD also makes detection from multiple feature maps. However, what they use are the directly shallow layers which have high resolution but not high enough semantic value to work for classification. What's worse, it slow down the speed significantly. **So SSD still works worse to dal with small objects.***

- Top-down pathway

  - After got the high semantic value of objects through deep layers in bottom-up pathway, we go down again to track back the locations of the objects in high resolution feature.

    ![img](https://cdn-images-1.medium.com/max/1600/1*XmNDHT8WWZbXACyBjg3ZeQ.jpeg)

  - But after all the downsampling and upsampling, this trace-back job is hard.

  - So FPN adds connections between reconstructed layers and the corresponding feature maps to help detector to predict the location better.

    ![img](https://cdn-images-1.medium.com/max/1600/1*aMRoAN7CtD1gdzTaZIT5gA.png)

The following is a detail diagram on the bottom-up and the top-down pathway. P2, P3, P4 and P5 are the pyramid of feature maps for object detection.

![img](https://cdn-images-1.medium.com/max/1600/1*ffxP_rL8-jMvipLhMJrVeA.png)

*Note that FPN is just for **extract better feature maps** waiting the object detector to detect objects.*

So following are some combinations with object detectors.

###### FPN with RPN

Feed each feature map to the object detector.

![img](https://cdn-images-1.medium.com/max/1600/1*cHR4YRqdPBOx4IDqzU-GwQ.png)

###### FPN with Fast R-CNN or Faster R-CNN

Just substitute the features maps with pyramid feature maps.

![img](https://cdn-images-1.medium.com/max/2400/1*Wvn0WG4XZ0w9Ed2fFYPrXw.jpeg)

##### Hard example mining when training most detectors

**Problem** - We often make more predictions than actual objects —> negative matches far more than positive matches —> class imbalance —> hurt the training 

**Solution** - (SSD)After classification, sort training examples by their calculated confidence loss and pick the top ones. Also make sure the negative-positive ration is at most 3:1. 

​		- (Focal loss see below)

This leads to a faster and more stable training.

##### Non-maximal suppression during inference

**Problem** - duplicate detections for the same object

**Solution** - use NMS to remove the duplications with lower confidence and only keep the one with highest confidence. 

1. Sort the predictions by the confidence score.

2. Go down the list one by one.

3. If the prediction

   - Has the same category with a previous prediction
   - And the IoU is greater than 0.5
     - IoU: Intersection of Union. 
     - ![Figure 2: Computing the Intersection of Union is as simple as dividing the area of overlap between the bounding boxes by the area of union (thank you to the excellent Pittsburg HW4 assignment for the inspiration for this figure).](https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png)

   Then, remove it.

##### Focal loss (RetinaNet)

As mentioned before in the *Hard example mining*, class imbalance hurts performance. SSD resamples to control the ratio. 

Focal loss(FL) is adjusted from cross-entropy loss(CE) for de-emphasizing the CE for high confidence loss and enhance the polarization:
$$
CE(p_t)=-\log(p_t) \\
FL(p_t)=-(1-p_t)^\gamma \log(p_t)
$$
Where $p_t$ is the predicted class probability for ground truth and $\gamma>0$.

![img](https://cdn-images-1.medium.com/max/1600/1*FCV96tP679EScoiwKq4IaQ.png)

###### RetinaNet 

ReinaNet is built on FPN and ResNet using the Focal loss.

![RetinaNet](https://cdn-images-1.medium.com/max/2400/1*jQFeF7gj6uCXVzUb08S9lg.png)